application {
    app-name-request = "RealtimeMemsqlPipelineRequest"
    app-name-event =   "RealtimeMemsqlPipelineEvent"
    memsqlhostname = "memsqlprod1"
    memsqlhostname_secondary = "memsqlprod2"
    memsqlport = 3306
    memsqlusername = "sparkuser"
    memsqlpassword = "s0@arKZ99"
    memsqldbname = "datawarehouse"
    memsqltablename = "EventsHourlyIM"
    statustableName = "EventLogStatus"
    sparkbatchinterval = 15
    request-driver-options = "-XX:+UseG1GC -Xms4g -Xmx8g -Dlog4j.configuration=file:/usr/local/spark-1.5.1-bin-hadoop2.6/conf/log4j_RequestLogDriver.properties"
    request-executor-options = "-XX:+UseG1GC -Dlog4j.configuration=file:/usr/local/spark-1.5.1-bin-hadoop2.6/conf/log4j_RequestLogExecutor.properties"
    event-driver-options = "-XX:+UseG1GC -Xms4g -Xmx8g -Dlog4j.configuration=file:/usr/local/spark-1.5.1-bin-hadoop2.6/conf/log4j_EventLogDriver.properties"
    event-executor-options = "-XX:+UseG1GC -Dlog4j.configuration=file:/usr/local/spark-1.5.1-bin-hadoop2.6/conf/log4j_EventLogExecutor.properties"
    spark-streaming-request=["spark.cores.max=40","spark.serializer=org.apache.spark.serializer.KryoSerializer", "spark.driver.memory=10g", "spark.executor.memory=15g", "spark.storage.memoryFraction=0.5",  "spark.driver.allowMultipleContexts=true", "spark.streaming.blockInterval=400ms", "spark.streaming.concurrentJobs=2", "spark.mesos.coarse=true", "spark.streaming.unpersist=true", "spark.sql.tungsten.enabled=true", "spark.sql.codegen=false", "spark.sql.unsafe.enabled=false" , "spark.streaming.kafka.maxRatePerPartition=25",  "spark.streaming.stopGracefullyOnShutdown=true"]
    spark-streaming-event=["spark.cores.max=8","spark.serializer=org.apache.spark.serializer.KryoSerializer", "spark.driver.memory=10g", "spark.executor.memory=15g", "spark.storage.memoryFraction=0.5",  "spark.driver.allowMultipleContexts=true", "spark.streaming.blockInterval=400ms", "spark.streaming.concurrentJobs=4", "spark.mesos.coarse=true", "spark.streaming.unpersist=true", "spark.sql.tungsten.enabled=true", "spark.sql.codegen=false", "spark.sql.unsafe.enabled=false" , "spark.streaming.kafka.maxRatePerPartition=10",  "spark.streaming.stopGracefullyOnShutdown=true"]
    spark-batch=["spark.executor.instances-10", "spark.executor.memory-6g", "spark.executor.userClassPathFirst-true", "spark.driver.userClassPathFirst-true"]
}